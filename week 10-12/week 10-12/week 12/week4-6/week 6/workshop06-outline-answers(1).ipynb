{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 6\n",
    "\n",
    "Starter code for workshop 6. You should have seen most of it before, but make sure you understand what it is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To plot even prettier figures\n",
    "import seaborn as sn\n",
    "\n",
    "# General data handling (pure numerics are better in numpy)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n",
      "(569, 31)\n"
     ]
    }
   ],
   "source": [
    "xarray = data.data\n",
    "yarray = data.target\n",
    "print(xarray.shape)\n",
    "print(yarray.shape)\n",
    "fullarray = np.concatenate((xarray,np.reshape(yarray,(-1,1))),axis=1)\n",
    "print(fullarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullarray[:,-1] = 1 - fullarray[:,-1]   # now invert the labels (so that malignant=1)\n",
    "df = pd.DataFrame(fullarray,columns = list(data.feature_names) + ['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code starts here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into separate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1\n",
    "#按70:15:15的比例将数据分成训练集、验证集和测试集。\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bigtrain_set, test_set = train_test_split(fullarray, test_size=0.15, random_state=42, stratify=fullarray[:,-1])\n",
    "train_set, val_set = train_test_split(bigtrain_set, test_size=0.15, random_state=42, stratify=bigtrain_set[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note the use of \"stratify\" in the calls above, as these make sure that each dataset has roughly the same proportions of the classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes are [(410, 30), (410,), (86, 30), (86,), (73, 30), (73,)]\n"
     ]
    }
   ],
   "source": [
    "X_train = train_set[:,:-1] #【：-1】= 从位置0到位置-1之前的数,也就是说去掉最后一个字符或者列的结果\n",
    "y_train = train_set[:,-1]\n",
    "X_test = test_set[:,:-1]\n",
    "y_test = test_set[:,-1]\n",
    "X_val = val_set[:,:-1]\n",
    "y_val = val_set[:,-1]\n",
    "print(f'Shapes are {[X_train.shape,y_train.shape,X_test.shape,y_test.shape,X_val.shape,y_val.shape]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37317073170731707 0.37209302325581395 0.3698630136986301\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(y_train),np.mean(y_test),np.mean(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这些是每个数据集中类的比例（因为类的值为0和1，所以平均值正好等于1表示的类的比例）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the proportions of the classes in each dataset (as classes are given values 0 and 1, so a mean is just equal to the proportion of the class represented by 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "preproc_pl = Pipeline([ ('imputer', SimpleImputer(strategy=\"median\")), \n",
    "                        ('std_scaler', StandardScaler()) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, hinge_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2 \n",
    "#在管道中，使用径向基函数（rbf）核、默认（超）参数构建SVM分类器，\n",
    "#并在验证集上确定该分类器的精度。\n",
    "\n",
    "svm_pl = Pipeline([('preproc',preproc_pl), ('svc',SVC(kernel='rbf'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9726027397260274\n"
     ]
    }
   ],
   "source": [
    "svm_pl.fit(X_train,y_train)\n",
    "y_val_pred = svm_pl.predict(X_val)\n",
    "acc = accuracy_score(y_val,y_val_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "X_mean = np.mean(X_train,axis=0)\n",
    "print(X_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3436379679.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/pc/6d606khj3jz573zj08smmc700000gn/T/ipykernel_42202/3436379679.py\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    fpr, tpr, thresholds = roc_curve(??, ??, pos_label=??)\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Q3\n",
    "#现在我们想建立一个简单的基线来比较准确度值，就像我们在早期的回归研讨会上所做的那样。\n",
    "#在这里，我们将分别为每个特征执行此操作，\n",
    "#通过使用公式将特征值转换为简单的“预测概率”：y_pred=（x-xmin）/（xmax-xmin），\n",
    "#该公式给出了0到1（包括）范围内的值。\n",
    "\n",
    "#从第一个特征开始，写一个循环，以在0到1之间的一组均匀间隔的值设置该预测的阈值（y_pred），\n",
    "#并为每个阈值计算精度。由此确定该功能的最大精确度（在所有测试阈值中）。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n in range(X_train.shape[1]):\n",
    "    xfeat = X_train[:,n]\n",
    "    xmin = np.min(xfeat)\n",
    "    xmax = np.max(xfeat)\n",
    "    y_pred = (xfeat-xmin)/(xmax-xmin)\n",
    "    fpr, tpr, thresholds = roc_curve(??, ??, pos_label=??)\n",
    "    aucval = auc(??,??)\n",
    "    if aucval<0.5: aucval = 1-aucval\n",
    "    acc = 0\n",
    "    for thr in xfeat:\n",
    "        acc = np.max([acc, accuracy_score(y_train, y_pred>thr)])\n",
    "    print(f'AUC for feature {n} = {aucval} ; Max accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [6, 4]:  # Best and worst feature numbers\n",
    "    xfeat = X_train[:,n]\n",
    "    xmin = np.min(xfeat)\n",
    "    xmax = np.max(xfeat)\n",
    "    y_pred = (xfeat-xmin)/(xmax-xmin)\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, y_pred, pos_label=1)\n",
    "    plt.plot(??,??,'b')\n",
    "    plt.title(f'ROC for feature {n}')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#请注意，最差的特征不是AUC最低的特征，而是AUC最接近0.5的特征，因为小于0.5的特征只是反向特征，可以被否定以获得正性能（AUC>0.5）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the worst feature is not the one with the lowest AUC, it is the one with the AUC closest to 0.5, as ones less than this are simply inverted features and could be negated to get positive performance (with AUC>0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "#从第3步中选择精度得分最高的两个功能。使用提供的代码（make_meshgrid和plot_contours）绘制步骤2中SVM分类器的决策边界\n",
    "#[请注意，这里提供的代码是scikit learn examplesLinks中一个外部站点的修改版本。]。我们将使用原始数据绘制这些决策边界，\n",
    "#因此在管道中传递到plot_等高线调用，而不仅仅是分类器部分。您需要根据原始特征值为make_meshgrid选择合适的范围。\n",
    "\n",
    "\n",
    "\n",
    "def make_meshgrid(x, y, ns=100):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on (only min and max used)\n",
    "    y: data to base y-axis meshgrid on (only min and max used)\n",
    "    ns: number of steps in grid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min(), x.max()\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    hx = (x_max - x_min)/ns\n",
    "    hy = (y_max - y_min)/ns\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max + hx, hx), np.arange(y_min, y_max + hy, hy))\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(clf, xx, yy, xmean, n1, n2, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    xmean : 1d array of mean values (to populate constant features with)\n",
    "    n1, n2: index numbers of features that change (for xx and yy)\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    # The following lines makes an MxN matrix to pass to the classifier (# samples x # features)\n",
    "    # It does this by multiplying Mx1 and 1xN matrices, where the former is filled with 1's\n",
    "    #  where M is the number of grid points in xx and N is the number of features in xmean\n",
    "    #  It is done in such a way that the xmean vector is replaced in each row\n",
    "    fullx = np.ones((xx.ravel().shape[0],1)) * np.reshape(xmean,(1,-1))\n",
    "    fullx[:,n1] = xx.ravel()\n",
    "    fullx[:,n2] = yy.ravel()\n",
    "    Z = clf.predict(fullx)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = plt.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2362937716.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/pc/6d606khj3jz573zj08smmc700000gn/T/ipykernel_42202/2362937716.py\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    x10, x90 = np.percentile(X_train[:,n0],[??,??])\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "\n",
    "#Q6\n",
    "#现在，在决策边界上方，以两个类的不同颜色显示训练数据点的散点图。\n",
    "#还可以使用相同的颜色但不同的符号添加验证数据点的散点图[提示：使用marker='s'获得正方形]。\n",
    "\n",
    "n0=6\n",
    "n1=7\n",
    "\n",
    "x10, x90 = np.percentile(X_train[:,n0],[??,??])\n",
    "y10, y90 = np.percentile(X_train[:,n1],[??,??])\n",
    "xx, yy = make_meshgrid(np.array([x10, x90]), np.array([y10, y90]), 500)\n",
    "\n",
    "plot_contours(svm_pl, xx, yy, X_mean, n0, n1, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X_train[??,??], X_train[??,??], c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n",
    "plt.scatter(X_val[??,??], X_val[??,??], c=??, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\", marker='s')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel(f\"Feature {n0}\")\n",
    "plt.ylabel(f\"Feature {n1}\")\n",
    "plt.title(\"Decision Boundary\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2180143251.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/pc/6d606khj3jz573zj08smmc700000gn/T/ipykernel_42202/2180143251.py\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    plt.scatter(X_train[??,??], X_train[??,??], c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Q7\n",
    "#使用多项式（“多边形”）和线性（“线性”）核重新运行SVM分类。比较结果的准确性和决策边界图。\n",
    "\n",
    "for kerneltype in ['linear','poly']:\n",
    "    print(f'SVM KERNEL = {kerneltype}')\n",
    "    svm_pl = Pipeline([('preproc',preproc_pl), ('svc',SVC(kernel=kerneltype))])\n",
    "    \n",
    "    svm_pl.fit(X_train,y_train)\n",
    "    y_val_pred = svm_pl.predict(X_val)\n",
    "    acc = accuracy_score(y_val,y_val_pred)\n",
    "    print(f'  Accuracy = {acc}')\n",
    "   \n",
    "    plot_contours(svm_pl, xx, yy, X_mean, n0, n1, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X_train[??,??], X_train[??,??], c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n",
    "    plt.scatter(X_val[??,??], X_val[??,??], c=??, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\", marker='s')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xlabel(f\"Feature {n0}\")\n",
    "    plt.ylabel(f\"Feature {n1}\")\n",
    "    plt.title(f\"Decision Boundary for kernel = {kerneltype}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3966769717.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/pc/6d606khj3jz573zj08smmc700000gn/T/ipykernel_42202/3966769717.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    acc = accuracy_score(y_??,y_val_pred)\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Recalculating the above results as they were not stored there, but if they were, this could be avoided\n",
    "for kerneltype in ['rbf','linear','poly']:\n",
    "    svm_pl = Pipeline([('preproc',preproc_pl), ('svc',SVC(kernel=kerneltype))])   \n",
    "    svm_pl.fit(X_train,y_train)\n",
    "    y_val_pred = svm_pl.predict(X_val)\n",
    "    acc = accuracy_score(y_??,y_val_pred)\n",
    "    print(f'Validation accuracy = {acc} for kernel {kerneltype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose RBF as best classifier as it has the best performance on the _validation_ set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2613289588.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/pc/6d606khj3jz573zj08smmc700000gn/T/ipykernel_42202/2613289588.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    svm_pl.fit(np.concatenate((X_train,X_val),axis=0),np.concatenate((y_train,y_??),axis=0))\u001b[0m\n\u001b[0m                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Q8\n",
    "#选择最佳分类器，并在测试集上报告结果。查看它与验证集结果的差异。\n",
    "\n",
    "kerneltype = 'rbf'\n",
    "svm_pl = Pipeline([('preproc',preproc_pl), ('svc',SVC(kernel=kerneltype))])  \n",
    "# Refit on combined training + validation set\n",
    "svm_pl.fit(np.concatenate((X_train,X_val),axis=0),np.concatenate((y_train,y_??),axis=0))\n",
    "# Evaluate this model (and only the selected best model) on the test set\n",
    "y_test_pred = svm_pl.predict(X_test)\n",
    "acc = accuracy_score(y_test,y_test_pred)\n",
    "print(f'Final test accuracy = {acc} for selected model: kernel={kerneltype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
